{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import SHS_data\n",
    "import util\n",
    "import paired_data\n",
    "\n",
    "reload(paired_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning cover song fingerprints\n",
    "\n",
    "### Data\n",
    "\n",
    "#### load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train, test, validation split\n",
    "ratio = (50,20,30)\n",
    "clique_dict, _ = SHS_data.read_cliques()\n",
    "train_cliques, test_cliques_big, _ = util.split_train_test_validation(clique_dict, ratio=ratio)\n",
    "\n",
    "# preload training data to memory (just about doable)\n",
    "print('Preloading training data...')\n",
    "train_uris = util.uris_from_clique_dict(train_cliques)\n",
    "chroma_dict = SHS_data.preload_chroma(train_uris)\n",
    "\n",
    "# make a training dataset of cover and non-cover pairs of songs\n",
    "print('Preparing training dataset...')\n",
    "n_patches, patch_len = 8, 64\n",
    "X_A, X_B, Y, pair_uris = paired_data.dataset_of_pairs(train_cliques, chroma_dict,\n",
    "                                                             n_patches=n_patches, patch_len=patch_len)\n",
    "print('    Training set:', X_A.shape, X_B.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load test data\n",
    "\n",
    "for now, load just a small part of the test set that we'll evaluate at every iteration, e.g., a few times batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick a test subset\n",
    "n_test_cliques = 50  # e.g., 50 ~ small actual datasets\n",
    "test_cliques = {uri: test_cliques_big[uri] for uri in test_cliques_big.keys()[:n_test_cliques]}\n",
    "\n",
    "# preload test data to memory (just about doable)\n",
    "print('Preloading test data...')\n",
    "test_uris = util.uris_from_clique_dict(test_cliques)\n",
    "chroma_dict_T = SHS_data.preload_chroma(test_uris)\n",
    "\n",
    "# make a test dataset of cover and non-cover pairs of songs\n",
    "print('Preparing test dataset...')\n",
    "X_A_T, X_B_T, Y_T, test_pair_uris_T = paired_data.dataset_of_pairs(test_cliques, chroma_dict_T,\n",
    "                                                             n_patches=n_patches, patch_len=patch_len)\n",
    "print('    Test set:', X_A_T.shape, X_B_T.shape, Y_T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "#### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_len = n_patches * patch_len\n",
    "\n",
    "x_A = tf.placeholder(\"float\", shape=[None, input_len, 12])\n",
    "x_B = tf.placeholder(\"float\", shape=[None, input_len, 12])\n",
    "y_ = tf.placeholder(\"float\", shape=[None,])\n",
    "\n",
    "x_image_A = tf.reshape(x_A, [-1, input_len, 12, 1])\n",
    "x_image_B = tf.reshape(x_B, [-1, input_len, 12, 1])\n",
    "y_ = tf.reshape(y_, [-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_layers = []\n",
    "\n",
    "# (512,12,1) > (128,1,32)\n",
    "list_of_layers.append((conv_layer, {'shape' : (1,12), 'n_filters' : 32, 'padding' : 'VALID'}))\n",
    "list_of_layers.append((max_pool_layer, {'shape' : (4,1)}))\n",
    "\n",
    "# (128,1,32) > (8,1,64) \n",
    "list_of_layers.append((conv_layer, {'shape' : (2,1)}))\n",
    "list_of_layers.append((max_pool_layer, {'shape' : (16,1)}))\n",
    "\n",
    "# (8,1,64) > (128)\n",
    "list_of_layers.append((fully_connected_layer, {'n_nodes' : 128}))\n",
    "\n",
    "net_A, net_B = build_butterfly_network(x_A, x_B, list_of_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "#### objective function\n",
    "\n",
    "Minize pair distances while maximizing non-pair distances smaller than `m`\n",
    "\n",
    "Following [1].\n",
    "\n",
    "1. Raffel, C., & Ellis, D. P. W. (2015). Large-Scale Content-Based Matching of Midi and Audio Files. Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), 234â€“240."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_I_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-99fb61fd98da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;31m# e.g., sqrt(256)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msquared_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_I_A\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout_I_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpair_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msquared_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnon_pair_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquared_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_I_A' is not defined"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "m = 10 # e.g., sqrt(256)\n",
    "\n",
    "squared_errors = tf.reduce_sum(tf.square(out_I_A - out_I_B), reduction_indices=1, keep_dims=True)\n",
    "pair_loss = tf.reduce_mean(y_ * squared_errors, name='pair_loss')\n",
    "non_pair_loss = tf.reduce_mean((1 - y_) * tf.square(tf.maximum(0.0, m - tf.sqrt(squared_errors))),\n",
    "                               name='non_pair_loss')\n",
    "\n",
    "loss_function = pair_loss + (alpha * non_pair_loss)\n",
    "loss_float = tf.cast(loss_function, \"float\", name='loss')\n",
    "\n",
    "# optimizer\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)\n",
    "# train_step = tf.train.RMSPropOptimizer(learning_rate, decay=0.9).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_epoques = 10 # 1250 ~ 5 x training set of 50% of all data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-1b6174f0f9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tensor_lr'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "learning_rate = tf.placeholder(tf.float32, shape=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reporting\n",
    "\n",
    "bundle all logging and printing into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def report(step, batch, train_log):\n",
    "    \n",
    "    if train_log is None:\n",
    "        n_epoques = 2500\n",
    "        metrics = ['train_error', 'test_error', 'train_bhatt', 'test_bhatt',\n",
    "                   'pair_loss', 'non_pair_loss', 'd_pairs', 'd_non_pairs']\n",
    "        train_log = pd.DataFrame(data = np.zeros((n_epoques, len(metrics))),\n",
    "                                 columns=metrics)\n",
    "    \n",
    "    # train and test feeds\n",
    "    train_feed = {x_A:batch[0], x_B:batch[1], y_: batch[2]}\n",
    "    test_feed = {x_A:X_A_T, x_B:X_B_T, y_: Y_T}\n",
    "    \n",
    "    # compute and log train metrics\n",
    "    train_metrics = sess.run([loss_float, pair_loss, non_pair_loss, squared_errors],\n",
    "                             feed_dict=train_feed)\n",
    "    train_log.loc[step, ['train_error', 'pair_loss', 'non_pair_loss']] = train_metrics[:-1]\n",
    "    more_train_metrics = approx_bhattacharyya(train_metrics[-1], train_feed[y_])\n",
    "    train_log.loc[step, ['train_bhatt', 'd_pairs', 'd_non_pairs']] = more_train_metrics\n",
    "    \n",
    "    # compute and log test metrics\n",
    "    train_log.loc[step, ['test_error']], test_squared_errors = sess.run([loss_float, squared_errors],\n",
    "                                                                        feed_dict=test_feed)\n",
    "    train_log.loc[step, ['test_bhatt']], _, _ = approx_bhattacharyya(test_squared_errors,\n",
    "                                                                     test_feed[y_])\n",
    "\n",
    "    # print some of the metrics\n",
    "    print(train_log[['train_error', 'test_error', 'train_bhatt',\n",
    "                     'test_bhatt', 'd_pairs', 'd_non_pairs']][step:])\n",
    "    \n",
    "    return train_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "n_epoques\n",
    "\n",
    "metrics = ['loss', 'paid_loss', 'non_pair_loss']\n",
    "train_log = report(n_epoques, )\n",
    "train_batches = get_batches([X_A, X_B, Y], batch_size=128)\n",
    "test_batch = [X_A_T, X_B_T, Y_T]\n",
    "for step in range(n_epoques):  \n",
    "    train_batch = next(train_batches)\n",
    "    if step%10 == 0:\n",
    "        train_log = report(train_log, step, train_batch=train_batch,\n",
    "                           test_batch=test_batch, metrics=metrics)\n",
    "    train_feed = {x_A:train_batch[0], x_B:train_batch[1],\n",
    "                  y_: train_batch[2], learning_rate: lr}\n",
    "    train_step.run(feed_dict=train_feed)\n",
    "train_log = report(train_log, '[end]', train_batch=batch,\n",
    "                   test_batch=test_batch, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot loss function \n",
    "\n",
    "plot loss functions for train and test data.\n",
    "\n",
    "Note that training loss fluctuates much more as it's computed for a different batch at every step, while test error is computed for the same (slightly larger) subset at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(train_error_log);\n",
    "plt.plot(error_pairs_log, color='g');\n",
    "plt.plot(error_non_pairs_log, color='r');\n",
    "plt.plot(test_error_log, color='k');\n",
    "plt.title('train (b) and test(k) loss function with train pairs (g) vs non-pairs (r) components');\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(d_pairs_log, color='g');\n",
    "plt.plot(d_non_pairs_log, color='r');\n",
    "plt.title('average distance, train pairs (g) vs non-pairs (r)');\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(np.log(train_bhatt_log));\n",
    "plt.plot(np.log(test_bhatt_log), 'k');\n",
    "plt.title('bhattacharyya distance train (b) and test (k)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pair_dists = np.sqrt(squared_dists_log[-1][np.where(labels_log[-1]==1)])\n",
    "non_pair_dists = np.sqrt(squared_dists_log[-1][np.where(labels_log[-1]==0)])\n",
    "L1 = pair_loss.eval(feed_dict={x_A:batch[0], x_B:batch[1], y_:batch[2]})\n",
    "L2 = non_pair_loss.eval(feed_dict={x_A:batch[0], x_B:batch[1], y_:batch[2]})\n",
    "\n",
    "bins = np.arange(0,10,0.4)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.hist(non_pair_dists, bins=bins, alpha=0.5);\n",
    "plt.hist(pair_dists, bins=bins, color='r', alpha=0.5);\n",
    "plt.subplot(143)\n",
    "plt.boxplot([non_pair_dists, pair_dists]);\n",
    "\n",
    "print('bhatt =', approx_bhattacharyya(squared_dists_log[-1], labels_log[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_squared_dists = squared_errors.eval(feed_dict={x_A:X_A_T, x_B:X_B_T})\n",
    "test_squared_dists = np.sum(test_squared_dists, axis=1)\n",
    "\n",
    "test_pair_dists = np.sqrt(test_squared_dists[np.where(Y_T==1)[0]])\n",
    "test_non_pair_dists = np.sqrt(test_squared_dists[np.where(Y_T==0)[0]])\n",
    "\n",
    "bins = np.arange(0,10,0.4)\n",
    "plt.figure()\n",
    "plt.hist(test_non_pair_dists, bins=bins, alpha=0.5);\n",
    "plt.hist(test_pair_dists, bins=bins, color='r', alpha=0.5);\n",
    "\n",
    "print('bhatt =', approx_bhattacharyya(test_squared_dists, Y_T.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # subset\n",
    "# test_subset = {uri: test_cliques[uri] for uri in test_cliques.keys()[:200]}\n",
    "\n",
    "# print('Preloading chroma...')\n",
    "# test_uris = util.uris_from_clique_dict(test_subset)\n",
    "# chroma_dict_T = SHS_data.preload_chroma(test_uris)\n",
    "\n",
    "# print('Collecting test dataset...')\n",
    "# X_A_T, X_B_T, is_cover_T, test_pair_uris_T = paired_data.dataset_of_pairs(test_subset, chroma_dict_T,\n",
    "#                                                              n_patches=n_patches, patch_len=patch_len)\n",
    "# print(X_A_T.shape, X_B_T.shape, is_cover_T.shape)\n",
    "\n",
    "# Y_T = is_cover_T.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_butterfly_network(x_A, x_B, [list_of_layers]):\n",
    "    network_A = [x_A]\n",
    "    network_B = [x_B]\n",
    "    for layer in list_of_layers:\n",
    "        network_A.append(layer['type'](network_A[-1], *layer['params']))\n",
    "        network_B.append(layer['type'](network_B[-1], *layer['params']))\n",
    "    return network_A, network_B\n",
    "\n",
    "list_of_layers = []\n",
    "list_of_layers.append(conv_bins, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
