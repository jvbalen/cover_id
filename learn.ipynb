{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'learn' from 'learn.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import IPython\n",
    "\n",
    "import SHS_data\n",
    "import util\n",
    "import paired_data\n",
    "\n",
    "import learn\n",
    "reload(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning cover song fingerprints\n",
    "\n",
    "This notebook contains experiments in which a fingerprint is learned from a dataset of cover songs. The main idea behind this is explained in our [Audio Bigrams](http://dspace.library.uu.nl/handle/1874/314940) paper [1].\n",
    "\n",
    "Very briefly explained:\n",
    "\n",
    "1. most fingerprints encode some kind of co-occurrence of salient events (e.g., Shazam, 'Intervalgram')\n",
    "2. 'salient event detection' can be implemented as a convolution:\n",
    "        conv2d(X, W)\n",
    "    with `W` the 'salient events'.\n",
    "3. co-occurrence can be implemented as\n",
    "        conv2d(X, w) @ X.T\n",
    "   with `w` a window and `@` the matrix product.\n",
    "4. all of this is differentiable, therefore, any fingerprinting system that can be formulated like this can be trained 'end-to-end'.\n",
    "\n",
    "To evaluate the learned fingerprint, we compare to the state-of-the-art '2D Fourier Transform Magniture Coeffients' by Bertin-Mahieux and Ellis [2], and a simpler fingerprinting approach by Kim et al [3].\n",
    "\n",
    "We use the [Second-hand Song Dataset](http://labrosa.ee.columbia.edu/millionsong/secondhand) with dublicates removed as proposed by [Julien Osmalskyj](http://www.montefiore.ulg.ac.be/~josmalskyj/code.php).\n",
    "\n",
    "[1] Van Balen, J., Wiering, F., & Veltkamp, R. (2015). [Audio Bigrams as a Unifying Model of Pitch-based Song Description](http://dspace.library.uu.nl/handle/1874/314940).\n",
    "\n",
    "[2] Bertin-Mahieux, T., & Ellis, D. P. W. (2012). [Large-Scale Cover Song Recognition Using The 2d Fourier Transform Magnitude.](http://academiccommons.columbia.edu/download/fedora_content/download/ac:159481/CONTENT/BertE12-2DFTM.pdf) In Proc. International Society for Music Information Retrieval Conference.\n",
    "\n",
    "[3] Kim, S., Unal, E., & Narayanan, S. (2008). [Music fingerprint extraction for classical music cover song identification.](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4607671&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4607671) IEEE Conference on Multimedia and Expo.\n",
    "\n",
    "\n",
    "\n",
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_patches, patch_len = 8, 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading training data...\n",
      "Preparing training dataset...\n",
      "    Training set: (32436, 512, 12) (32436, 512, 12) (32436,)\n"
     ]
    }
   ],
   "source": [
    "# train, test, validation split\n",
    "ratio = (50,20,30)\n",
    "clique_dict, _ = SHS_data.read_cliques()\n",
    "train_cliques, test_cliques_big, _ = util.split_train_test_validation(clique_dict, ratio=ratio)\n",
    "\n",
    "# preload training data to memory (just about doable)\n",
    "print('Preloading training data...')\n",
    "train_uris = util.uris_from_clique_dict(train_cliques)\n",
    "chroma_dict = SHS_data.preload_chroma(train_uris)\n",
    "\n",
    "# make a training dataset of cover and non-cover pairs of songs\n",
    "print('Preparing training dataset...')\n",
    "X_A, X_B, Y, pair_uris = paired_data.dataset_of_pairs(train_cliques, chroma_dict,\n",
    "                                                             n_patches=n_patches, patch_len=patch_len)\n",
    "print('    Training set:', X_A.shape, X_B.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data\n",
    "\n",
    "For now, load just a small part of the test set that we'll evaluate at every iteration, e.g., a few times batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading test data...\n",
      "Preparing test dataset...\n",
      "    Test set: (340, 512, 12) (340, 512, 12) (340,)\n"
     ]
    }
   ],
   "source": [
    "# pick a test subset\n",
    "n_test_cliques = 50  # e.g., 50 ~ small actual datasets\n",
    "test_cliques = {uri: test_cliques_big[uri] for uri in test_cliques_big.keys()[:n_test_cliques]}\n",
    "\n",
    "# preload test data to memory (just about doable)\n",
    "print('Preloading test data...')\n",
    "test_uris = util.uris_from_clique_dict(test_cliques)\n",
    "chroma_dict_T = SHS_data.preload_chroma(test_uris)\n",
    "\n",
    "# make a test dataset of cover and non-cover pairs of songs\n",
    "print('Preparing test dataset...')\n",
    "X_A_T, X_B_T, Y_T, test_pair_uris_T = paired_data.dataset_of_pairs(test_cliques, chroma_dict_T,\n",
    "                                                         n_patches=n_patches, patch_len=patch_len)\n",
    "print('    Test set:', X_A_T.shape, X_B_T.shape, Y_T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network\n",
    "\n",
    "Set up a siamese network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for repeated runs with different networks\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make network\n",
    "network = learn.siamese_network(input_shape=(n_patches*patch_len, 12))\n",
    "network.add_conv_layer(shape=(1,12), n_filters=24, padding='VALID')\n",
    "network.add_matmul_layer(filter_len=8, n_filters=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Set training parameters and run for `n_epoque` iterations.\n",
    "Current implementation requires an 'interactive session'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 4\n",
    "m = 10\n",
    "lr = 3e-4\n",
    "batch_size = 100\n",
    "n_epoques = 3200  # train set ~ 320 x 100\n",
    "\n",
    "# training metrics\n",
    "loss, pair_loss, non_pair_loss = network.loss(m=m, alpha=alpha)\n",
    "bhatt, d_pairs, d_non_pairs = network.bhattacharyya()\n",
    "\n",
    "# optimiser\n",
    "train_step = network.train_step(loss, learning_rate=lr)\n",
    "\n",
    "# choose which metrics to log\n",
    "metrics = [loss, d_pairs, d_non_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start Tensorflow session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c1d21c7cdc7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_batches' is not defined"
     ]
    }
   ],
   "source": [
    "# train\n",
    "for step in range(n_epoques):  \n",
    "    train_batch = next(train_batches)\n",
    "    \n",
    "    # report\n",
    "    network.log_errors(sess, train_batch=train_batch,\n",
    "                       test_batch=test_batch, metrics=metrics, log_every=10)\n",
    "    \n",
    "    # train\n",
    "    train_feed = {network.x_A:train_batch[0], network.x_B:train_batch[1],\n",
    "                  network.is_cover:train_batch[2]}\n",
    "    train_step.run(feed_dict=train_feed)\n",
    "    \n",
    "# report final\n",
    "network.log_errors(sess, train_batch=train_batch,\n",
    "                   test_batch=test_batch, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot train and test outcome\n",
    "\n",
    "#### loss functions\n",
    "\n",
    "Note that training loss fluctuates much more as it's computed for a different batch at every step, while test error is (currently) computed for the same (slightly larger) subset at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(network.train_log['TR.loss']);\n",
    "plt.plot(network.train_log['TE.loss'], color='k');\n",
    "plt.title('train (b) and test (k) loss function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_distances(feed):\n",
    "    y_A, y_B = sess.run([network.subnet_A[-1], network.subnet_B[-1]],\n",
    "                                  feed_dict=feed)\n",
    "    is_cover = feed[network.is_cover]\n",
    "\n",
    "    pair_dists = np.sqrt(np.sum((y_A - y_B)**2, axis=1))[np.where(is_cover==1)]\n",
    "    non_pair_dists = np.sqrt(np.sum((y_A - y_B)**2, axis=1))[np.where(is_cover==0)]\n",
    "\n",
    "    bins = np.arange(0,20,0.5)\n",
    "    plt.figure(figsize=(16,4))\n",
    "    plt.subplot(121)\n",
    "    plt.hist(non_pair_dists, bins=bins, alpha=0.5);\n",
    "    plt.hist(pair_dists, bins=bins, color='r', alpha=0.5);\n",
    "    plt.subplot(143)\n",
    "    plt.boxplot([non_pair_dists, pair_dists]);\n",
    "    \n",
    "# train distances\n",
    "train_feed = {network.x_A:train_batch[0], network.x_B:train_batch[1], network.is_cover: train_batch[2]}\n",
    "plot_distances(train_feed)\n",
    "\n",
    "# test distances\n",
    "test_feed = {network.x_A: X_A_T, network.x_B: X_B_T, network.is_cover: Y_T}\n",
    "plot_distances(test_feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test fingerprint\n",
    "\n",
    "Test the learned fingerprint and compare to other impelementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import main\n",
    "import fingerprints as fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def fingerprint(chroma, n_patches=8, patch_len=64):\n",
    "#     n_frames, n_bins = chroma.shape\n",
    "#     if not n_frames == n_patches * patch_len:\n",
    "#         chroma = paired_data.patchwork(chroma, n_patches=n_patches,\n",
    "#                                        patch_len=patch_len)\n",
    "#     fps = []\n",
    "#     for i in range(12):\n",
    "#         chroma_trans = np.roll(chroma, -i, axis=1)\n",
    "#         chroma_tensor = chroma_trans.reshape((1, n_patches*patch_len, 12))\n",
    "#         network_out = network.subnet_A[-1]\n",
    "#         fp = network_out.eval(feed_dict={network.x_A : chroma_tensor})\n",
    "#         fps.append(fp.flatten())\n",
    "#     return fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### covariance-based fingerprint\n",
    "\n",
    "Kim, S., Unal, E., & Narayanan, S. (2008). Music fingerprint extraction for classical music cover song identification.\n",
    "IEEE Conference on Multimedia and Expo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = main.run_leave_one_out_experiment(test_cliques,\n",
    "                                            fp_function=fp.cov,\n",
    "                                            print_every=50)\n",
    "print('results:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2d-DFT-based fingerprint\n",
    "\n",
    "Bertin-Mahieux, T., & Ellis, D. P. W. (2012). Large-Scale Cover Song Recognition Using The 2d Fourier Transform Magnitude. In Proc. International Society for Music Information Retrieval Conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = main.run_leave_one_out_experiment(test_cliques,\n",
    "                                            fp_function=fp.fourier,\n",
    "                                            print_every=50)\n",
    "print('results:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learned fingerprint\n",
    "\n",
    "We now see that with the right configuration, we are able make the fingerprinter do better than the 2d-DFT-based fingerprints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = main.run_leave_one_out_experiment(test_cliques,\n",
    "                                            fp_function=network.fingerprint,\n",
    "                                            print_every=50)\n",
    "print('fp_results:', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
