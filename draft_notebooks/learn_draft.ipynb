{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import SHS_data\n",
    "import util\n",
    "import paired_data\n",
    "\n",
    "reload(paired_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Learning Cover Song Fingerprints\n",
    "\n",
    "This notebook documents an experiment in which we try to learn a fingerprinting function for chroma-based cover song retrieval.\n",
    "\n",
    "It relies on three other modules in this project for data handling: `SHS_data` (to load Second Hand Song chroma features and ground truth), `paired_data` (to preprocess data for training a neural network) and `util`.\n",
    "\n",
    "The cover detection experiment in the next section uses `main` for the experiment routines and `fingerprints`, which contains other fingerprinting functions to which we can compare.\n",
    "\n",
    "But first, this section presents the fingerprint learning, in three parts:\n",
    "\n",
    "1. Data - loading train and test data to memory\n",
    "\n",
    "1. Network components - defining network variables and layers\n",
    "    \n",
    "1. Network - setting up a three-layer convolutional neural network\n",
    "\n",
    "1. Training - training (and testing) the network\n",
    "\n",
    "### 1. Data\n",
    "\n",
    "#### load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading training data...\n",
      "Preparing training dataset...\n",
      "    Training set: (32436, 512, 12) (32436, 512, 12) (32436,)\n"
     ]
    }
   ],
   "source": [
    "# train, test, validation split\n",
    "ratio = (50,20,30)\n",
    "clique_dict, _ = SHS_data.read_cliques()\n",
    "train_cliques, test_cliques_big, _ = util.split_train_test_validation(clique_dict, ratio=ratio)\n",
    "\n",
    "# preload training data to memory (just about doable)\n",
    "print('Preloading training data...')\n",
    "train_uris = util.uris_from_clique_dict(train_cliques)\n",
    "chroma_dict = SHS_data.preload_chroma(train_uris)\n",
    "\n",
    "# make a training dataset of cover and non-cover pairs of songs\n",
    "print('Preparing training dataset...')\n",
    "n_patches, patch_len = 8, 64\n",
    "X_A, X_B, Y, pair_uris = paired_data.dataset_of_pairs(train_cliques, chroma_dict,\n",
    "                                                             n_patches=n_patches, patch_len=patch_len)\n",
    "print('    Training set:', X_A.shape, X_B.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load test data\n",
    "\n",
    "for now, load just a small part of the test set that we'll evaluate at every iteration, e.g., a few times batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading test data...\n",
      "Preparing test dataset...\n",
      "    Test set: (340, 512, 12) (340, 512, 12) (340,)\n"
     ]
    }
   ],
   "source": [
    "# pick a test subset\n",
    "n_test_cliques = 50  # e.g., 50 ~ small actual datasets\n",
    "test_cliques = {uri: test_cliques_big[uri] for uri in test_cliques_big.keys()[:n_test_cliques]}\n",
    "\n",
    "# preload test data to memory (just about doable)\n",
    "print('Preloading test data...')\n",
    "test_uris = util.uris_from_clique_dict(test_cliques)\n",
    "chroma_dict_T = SHS_data.preload_chroma(test_uris)\n",
    "\n",
    "# make a test dataset of cover and non-cover pairs of songs\n",
    "print('Preparing test dataset...')\n",
    "X_A_T, X_B_T, Y_T, test_pair_uris_T = paired_data.dataset_of_pairs(test_cliques, chroma_dict_T,\n",
    "                                                             n_patches=n_patches, patch_len=patch_len)\n",
    "print('    Test set:', X_A_T.shape, X_B_T.shape, Y_T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Network Components\n",
    "\n",
    "#### variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_scale = 0.1\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=weight_scale)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(weight_scale, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolutional layers\n",
    "def conv_bins(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def conv_frames(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# max pool layers\n",
    "def max_pool_4x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 4, 1, 1],\n",
    "                          strides=[1, 4, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_8x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 8, 1, 1],\n",
    "                          strides=[1, 8, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_16x1(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 16, 1, 1],\n",
    "                          strides=[1, 16, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Network\n",
    "\n",
    "#### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_len = n_patches * patch_len\n",
    "\n",
    "x_A = tf.placeholder(\"float\", shape=[None, input_len, 12])\n",
    "x_B = tf.placeholder(\"float\", shape=[None, input_len, 12])\n",
    "y_ = tf.placeholder(\"float\", shape=[None,])\n",
    "\n",
    "x_image_A = tf.reshape(x_A, [-1, input_len, 12, 1])\n",
    "x_image_B = tf.reshape(x_B, [-1, input_len, 12, 1])\n",
    "y_ = tf.reshape(y_, [-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conv layer 1\n",
    "\n",
    "`(512, 12, 1) > (128, 1, 32)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([1, 12, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1_A = tf.nn.relu(conv_bins(x_image_A, W_conv1) + b_conv1)\n",
    "h_conv1_B = tf.nn.relu(conv_bins(x_image_B, W_conv1) + b_conv1)\n",
    "\n",
    "h_pool1_A = max_pool_4x1(h_conv1_A)\n",
    "h_pool1_B = max_pool_4x1(h_conv1_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conv layer 2\n",
    "\n",
    "`(128, 1, 32) > (8, 1, 64)`\n",
    "\n",
    "`16x1` max-pooling is used to pool at once across the last convolution (`8x1`) and the `8` patches of chroma of which the input features is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([2, 1, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2_A = tf.nn.relu(conv_frames(h_pool1_A, W_conv2) + b_conv2)\n",
    "h_conv2_B = tf.nn.relu(conv_frames(h_pool1_B, W_conv2) + b_conv2)\n",
    "\n",
    "h_pool2_A = max_pool_16x1(h_conv2_A)\n",
    "h_pool2_B = max_pool_16x1(h_conv2_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fully connected layer\n",
    "\n",
    "`(8, 1, 64) > (128)`\n",
    "\n",
    "max-pool over patches and flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_pool2_A_flat = tf.reshape(h_pool2_A, [-1, 8*64])  # flatten images first\n",
    "h_pool2_B_flat = tf.reshape(h_pool2_B, [-1, 8*64])\n",
    "\n",
    "W_fc1 = weight_variable([8*64, 128])\n",
    "b_fc1 = bias_variable([128])\n",
    "\n",
    "out_I_A = tf.tanh(tf.matmul(h_pool2_A_flat, W_fc1) + b_fc1)\n",
    "out_I_B = tf.tanh(tf.matmul(h_pool2_B_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# h_fc1_A = tf.nn.relu(tf.matmul(h_pool2_A_flat, W_fc1) + b_fc1)\n",
    "# h_fc1_B = tf.nn.relu(tf.matmul(h_pool2_B_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# out_I_A = tf.tanh(h_pool2_A_flat)\n",
    "# out_I_B = tf.tanh(h_pool2_B_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bhattacharyya distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def approx_bhattacharyya(squared_dists, is_cover):\n",
    "    \"\"\"Approximate Bhattacharyya distance between cover and non-cover distances.\n",
    "    \n",
    "    Similar to Mahalanobis distance, but for distributions with different variances.\n",
    "    Assumes normality, hence approximate (distances are bound by 0).\n",
    "    \"\"\"\n",
    "    pair_dists = np.sqrt(squared_dists[np.where(is_cover==1)])\n",
    "    non_pair_dists = np.sqrt(squared_dists[np.where(is_cover==0)])\n",
    "    \n",
    "    mu_pairs, sigma2_pairs = np.mean(pair_dists), np.var(pair_dists)\n",
    "    mu_non_pairs, sigma2_non_pairs = np.mean(non_pair_dists), np.var(non_pair_dists)\n",
    "\n",
    "    bhatt = (0.25 * np.log(0.25 * (sigma2_pairs/sigma2_non_pairs + sigma2_non_pairs/sigma2_pairs + 2)) +\n",
    "             0.25 * (mu_pairs - mu_non_pairs)**2 / (sigma2_pairs + sigma2_non_pairs))\n",
    "    return bhatt, mu_pairs, mu_non_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training\n",
    "\n",
    "#### objective function\n",
    "\n",
    "Minize pair distances while maximizing non-pair distances smaller than `m`\n",
    "\n",
    "Following [1].\n",
    "\n",
    "1. Raffel, C., & Ellis, D. P. W. (2015). Large-Scale Content-Based Matching of Midi and Audio Files. Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR), 234–240."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "m = 10\n",
    "\n",
    "squared_errors = tf.reduce_sum(tf.square(out_I_A - out_I_B), reduction_indices=1, keep_dims=True)\n",
    "pair_loss = tf.reduce_mean(y_ * squared_errors)\n",
    "non_pair_loss = tf.reduce_mean((1 - y_) * tf.square(tf.maximum(0.0, m - tf.sqrt(squared_errors))))\n",
    "\n",
    "loss_function = pair_loss + (alpha * non_pair_loss)\n",
    "# loss_float = tf.cast(loss_function, \"float\")\n",
    "\n",
    "# optimizer\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init basic logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_pairs_log = []\n",
    "error_non_pairs_log = []\n",
    "\n",
    "squared_dists_log = []\n",
    "labels_log = []\n",
    "d_pairs_log = []\n",
    "d_non_pairs_log = []\n",
    "\n",
    "train_error_log = []\n",
    "train_bhatt_log = []\n",
    "\n",
    "test_error_log = []\n",
    "test_bhatt_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reporting\n",
    "\n",
    "bundle all logging and printing into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def report(step, batch):\n",
    "    print('step {}'.format(step))\n",
    "\n",
    "    # train and test feeds\n",
    "    train_feed = {x_A:batch[0], x_B:batch[1], y_: batch[2]}\n",
    "    test_feed = {x_A:X_A_T, x_B:X_B_T, y_: Y_T}\n",
    "    \n",
    "    # train metrics\n",
    "    error_pairs, error_non_pairs, squared_dists, train_error = sess.run([pair_loss, non_pair_loss,\n",
    "                                                                        squared_errors, loss_float],\n",
    "                                                                        feed_dict=train_feed)\n",
    "    train_bhatt, d_pairs, d_non_pairs = approx_bhattacharyya(squared_dists, train_feed[y_])\n",
    "    \n",
    "    # test metrics\n",
    "    test_squared_dists, test_error = sess.run([squared_errors, loss_float],\n",
    "                                              feed_dict=test_feed)\n",
    "    test_bhatt, _, _ = approx_bhattacharyya(test_squared_dists, test_feed[y_])\n",
    "\n",
    "    # log all metrics\n",
    "    error_pairs_log.append(error_pairs)\n",
    "    error_non_pairs_log.append(error_non_pairs)\n",
    "    squared_dists_log.append(squared_dists)\n",
    "    labels_log.append(batch[2])\n",
    "    d_non_pairs_log.append(d_non_pairs)\n",
    "    d_pairs_log.append(d_pairs)\n",
    "    train_bhatt_log.append(train_bhatt)\n",
    "    train_error_log.append(train_error)\n",
    "    test_bhatt_log.append(test_bhatt)    \n",
    "    test_error_log.append(test_error)\n",
    "    \n",
    "    # print some metrics\n",
    "    print('  d_pairs, d_non_pairs = %.3g, %.3g' % (d_pairs, d_non_pairs))\n",
    "    print('  train error %.3g, train bhatt %.3g' % (train_error, train_bhatt))\n",
    "    print('  test error %.3g, test bhatt %.3g' % (test_error, test_bhatt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError() in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x12f110c90>> ignored\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Placeholder', defined at:\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-88b1ad4a72a9>\", line 3, in <module>\n    x_A = tf.placeholder(\"float\", shape=[None, input_len, 12])\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 742, in placeholder\n    name=name)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 583, in _placeholder\n    name=name)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2040, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1087, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-c27d6faaf0c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_feed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_A\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_B\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-1909f8181a34>\u001b[0m in \u001b[0;36mreport\u001b[0;34m(step, batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m     error_pairs, error_non_pairs, squared_dists, train_error = sess.run([pair_loss, non_pair_loss,\n\u001b[1;32m     10\u001b[0m                                                                         squared_errors, loss_float],\n\u001b[0;32m---> 11\u001b[0;31m                                                                         feed_dict=train_feed)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_bhatt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_non_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapprox_bhattacharyya\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquared_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_feed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 511\u001b[0;31m                            feed_dict_string)\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 564\u001b[0;31m                            target_list)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         raise errors._make_specific_exception(node_def, op, error_message,\n\u001b[0;32m--> 586\u001b[0;31m                                               e.code)\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nCaused by op u'Placeholder', defined at:\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2723, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2825, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-88b1ad4a72a9>\", line 3, in <module>\n    x_A = tf.placeholder(\"float\", shape=[None, input_len, 12])\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 742, in placeholder\n    name=name)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 583, in _placeholder\n    name=name)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2040, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Jan/anaconda/envs/tf_env/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1087, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "n_epoques = 2500    # 2500 ~ 10 x training set after (50,20,30) split\n",
    "batch_size = 100\n",
    "lr = 3e-4\n",
    "\n",
    "train_batches = paired_data.get_batches([X_A, X_B, Y], batch_size=batch_size)\n",
    "for step in range(n_epoques):  \n",
    "    batch = next(train_batches)\n",
    "    if step%10 == 0:\n",
    "        report(step, batch)\n",
    "    train_feed = {x_A:batch[0], x_B:batch[1], y_: batch[2], learning_rate: lr}\n",
    "    train_step.run(feed_dict=train_feed)\n",
    "report('[end]', batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "typical result:\n",
    "\n",
    "    # step [end]\n",
    "    #  d_pairs, d_non_pairs = 3.47, 5.71\n",
    "    #  train error 17.2, train bhatt 0.335\n",
    "    #  test error 19.5, test bhatt 0.155\n",
    "\n",
    "#### plot loss function \n",
    "\n",
    "plot loss functions for train and test data.\n",
    "\n",
    "**Note**: test error is computed for the same subset at every step, making it appear much more stable than the training loss (computed for a different batch at every step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.plot(train_error_log);\n",
    "plt.plot(error_pairs_log, color='g');\n",
    "plt.plot(error_non_pairs_log, color='r');\n",
    "plt.plot(test_error_log, color='k');\n",
    "plt.title('train (b) and test(k) loss function with train pairs (g) vs non-pairs (r) components');\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(d_pairs_log, color='g');\n",
    "plt.plot(d_non_pairs_log, color='r');\n",
    "plt.title('average distance, train pairs (g) vs non-pairs (r)');\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(np.log(train_bhatt_log));\n",
    "plt.plot(np.log(test_bhatt_log), 'k');\n",
    "plt.title('bhattacharyya distance train (b) and test (k)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pair_dists = np.sqrt(squared_dists_log[-1][np.where(labels_log[-1]==1)])\n",
    "non_pair_dists = np.sqrt(squared_dists_log[-1][np.where(labels_log[-1]==0)])\n",
    "L1 = pair_loss.eval(feed_dict={x_A:batch[0], x_B:batch[1], y_:batch[2]})\n",
    "L2 = non_pair_loss.eval(feed_dict={x_A:batch[0], x_B:batch[1], y_:batch[2]})\n",
    "\n",
    "bins = np.arange(0,10,0.4)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.hist(non_pair_dists, bins=bins, alpha=0.5);\n",
    "plt.hist(pair_dists, bins=bins, color='r', alpha=0.5);\n",
    "plt.subplot(143)\n",
    "plt.boxplot([non_pair_dists, pair_dists]);\n",
    "\n",
    "print('bhatt =', approx_bhattacharyya(squared_dists_log[-1], labels_log[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_squared_dists = squared_errors.eval(feed_dict={x_A:X_A_T, x_B:X_B_T})\n",
    "test_squared_dists = np.sum(test_squared_dists, axis=1)\n",
    "\n",
    "test_pair_dists = np.sqrt(test_squared_dists[np.where(Y_T==1)[0]])\n",
    "test_non_pair_dists = np.sqrt(test_squared_dists[np.where(Y_T==0)[0]])\n",
    "\n",
    "bins = np.arange(0,10,0.4)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.hist(test_non_pair_dists, bins=bins, alpha=0.5);\n",
    "plt.hist(test_pair_dists, bins=bins, color='r', alpha=0.5);\n",
    "plt.subplot(143)\n",
    "plt.boxplot([test_non_pair_dists, test_pair_dists]);\n",
    "\n",
    "print('bhatt =', approx_bhattacharyya(test_squared_dists, Y_T.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Test Experiment\n",
    "\n",
    "Run a cover detection experiment on some of the Second Hand Song data, using the modules implemented before. We simply pass `fingerprint()`, a wrapper function around `out_I_A.eval()`, to `main.run_leave_one_out_experiment()`.\n",
    "\n",
    "First, however, we compute some baseline performances using existing fingerprinting methods (see `fingerprints` documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import main\n",
    "import fingerprints as fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Results for static fingerprints\n",
    "\n",
    "**Note**: there are some issues with short chroma files when computing the fingerprinting functions in `fingerprints`  for the complete 20% test set.\n",
    "For now, we will be using only the very short dataset that is `test_cliques`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = main.run_leave_one_out_experiment(test_cliques,\n",
    "                                            fp_function=fp.cov,\n",
    "                                            print_every=50)\n",
    "print('results:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = main.run_leave_one_out_experiment(test_cliques,\n",
    "                                            fp_function=fp.fourier,\n",
    "                                            print_every=50)\n",
    "print('results:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `fp.cov`\n",
    "\n",
    "results: {'mean r5': 0.22659574468085109, 'mean ap': 0.19774902881386611, 'mean p1': 0.14184397163120568}\n",
    "\n",
    "#### `fp.fourier`\n",
    "\n",
    "results: {'mean r5': 0.3851063829787234, 'mean ap': 0.35797756949792087, 'mean p1': 0.39007092198581561}\n",
    "\n",
    "### 2. Results for learned fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fingerprint(chroma, n_patches=8, patch_len=64):\n",
    "    patchwork = paired_data.patchwork(chroma, n_patches=n_patches,\n",
    "                                      patch_len=patch_len)\n",
    "    fps = []\n",
    "    for i in range(12):\n",
    "        patchwork_trans = np.roll(patchwork, -i, axis=1)\n",
    "        patchwork_tensor = patchwork_trans.reshape((1, n_patches*patch_len, 12))\n",
    "        fp = sess.run(out_I_A, feed_dict={x_A : patchwork_tensor})\n",
    "        fps.append(fp.flatten())\n",
    "    return fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = main.run_leave_one_out_experiment(test_cliques,\n",
    "                                            fp_function=fingerprint,\n",
    "                                            print_every=50)\n",
    "print('results:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1x12x32-2x1x64-128 network\n",
    "\n",
    "results: {'mean r5': 0.22872340425531915, 'mean ap': 0.21106169953170131, 'mean p1': 0.1773049645390071}\n",
    "\n",
    "#### conclusion\n",
    "\n",
    "- *outperforms covariance-based fingerprints*\n",
    "    (most clearly in terms of precision)\n",
    "\n",
    "- *but not fingerprints based on the 2D-fourier-transform*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
